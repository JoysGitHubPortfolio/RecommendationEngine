{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1wfwpkH3sbu5SJcK30k1FP-hpl6cGBc5I",
      "authorship_tag": "ABX9TyPhN6sG4RBPsrIPfLGeevF9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoysGitHubPortfolio/RecommendationEngine/blob/main/Joy_Cowper_Muzz_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import Dependencies"
      ],
      "metadata": {
        "id": "nBAJrSuJc4WC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install implicit\n",
        "!pip install pyngrok"
      ],
      "metadata": {
        "collapsed": true,
        "id": "juDPiw6F0d6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsdIs-sAMeuI"
      },
      "outputs": [],
      "source": [
        "# Data processing and visuals\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine Learning\n",
        "from scipy.sparse import csr_matrix\n",
        "from implicit.als import AlternatingLeastSquares\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Model Serving\n",
        "from flask import Flask, request, jsonify\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "\n",
        "# Environment Handling\n",
        "from google.colab import userdata\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ingestion"
      ],
      "metadata": {
        "id": "VgR2bBvJdBir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls drive/MyDrive/Muzz/"
      ],
      "metadata": {
        "id": "txPyGwJJPEkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip drive/MyDrive/Muzz/swipes.zip -d swipes_data"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1CtKBSg3b5WE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"swipes_data/swipes.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "i_IYoDNPcEtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df)"
      ],
      "metadata": {
        "id": "YDWm8OcheGg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA"
      ],
      "metadata": {
        "id": "oM1PK7d0gNN_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Schema Statistics"
      ],
      "metadata": {
        "id": "Mr7wQYkRaIiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"like\"].mean()"
      ],
      "metadata": {
        "id": "K9PmJjEmWdKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_users = df[[\"decidermemberid\", \"decidergender\"]].drop_duplicates().groupby(\"decidergender\").size()\n",
        "unique_users"
      ],
      "metadata": {
        "id": "YDdCDwHTjAai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleansing"
      ],
      "metadata": {
        "id": "tqQEA9HEaS9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Observe how many same-gender interactions occur (not valid by front-end constraints)\n",
        "ff_rows = len(df.loc[(df['decidergender'] == 'F') & (df['othergender'] == 'F')])\n",
        "mm_rows = len(df.loc[(df['decidergender'] == 'M') & (df['othergender'] == 'M')])\n",
        "print(ff_rows, mm_rows)"
      ],
      "metadata": {
        "id": "F2pUl0Xidtcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask_valid = (\n",
        "    (df[\"decidergender\"] == \"M\") & (df[\"othergender\"] == \"F\")\n",
        ") | (\n",
        "    (df[\"decidergender\"] == \"F\") & (df[\"othergender\"] == \"M\")\n",
        ")\n",
        "\n",
        "df = df[mask_valid].copy(deep=True)"
      ],
      "metadata": {
        "id": "WGl2vYhpgpGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "like_rates = df.groupby(['decidergender', 'decidermemberid']).agg(num_swipes = ('like', 'size'),\n",
        "                                                                  num_likes = ('like', 'sum'),\n",
        "                                                                  like_rate = ('like', 'mean')).reset_index()\n",
        "\n",
        "like_rates = like_rates[like_rates['like_rate'] > 0].copy(deep=True) # effectively filter out inactive users\n",
        "like_rates = like_rates[like_rates['like_rate'] < 1].copy(deep=True) # effectively filter out spamming users\n",
        "\n",
        "print('Number of Valid Users:', len(like_rates), '\\n')\n",
        "like_rates[0:5]"
      ],
      "metadata": {
        "id": "Sasq9HqO8sqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualisation"
      ],
      "metadata": {
        "id": "kpYWAsM6_gOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_my_member_dists(member_counts: pd.DataFrame = like_rates,\n",
        "                         variable: str = 'num_swipes',\n",
        "                         log: bool = False\n",
        "                         ):\n",
        "\n",
        "  fig, axes = plt.subplots(1, 2, figsize=(6, 2.5),\n",
        "                           sharey=True,\n",
        "                           constrained_layout=True)\n",
        "  for ax, gender in zip(axes, ['M', 'F']):\n",
        "      x = member_counts.loc[member_counts['decidergender'] == gender, variable]\n",
        "\n",
        "      if log:\n",
        "        x = np.log1p(x) # use log scale and 1+probability to normalise range of values\n",
        "        log_title = 'log scale'\n",
        "      else:\n",
        "        log_title = ''\n",
        "\n",
        "      if variable == 'num_swipes':\n",
        "        ax.set_xlim(0,1000)\n",
        "        bins=500\n",
        "      if variable == 'num_likes':\n",
        "        ax.set_xlim(0,250)\n",
        "        bins=250\n",
        "      if variable == 'like_rate':\n",
        "        ax.set_xlim(0,1)\n",
        "        bins=250\n",
        "\n",
        "      ax.hist(x, bins=bins, density=True)\n",
        "      x.plot(kind='kde',\n",
        "            ax=ax,\n",
        "            bw_method=0.25)\n",
        "\n",
        "      ax.set_title(f'{gender} - Distribution:\\n{log_title}')\n",
        "      ax.set_xlabel(f'{variable}')\n",
        "      ax.set_ylabel('Probability Density')\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "7BOqgCUxhK-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_my_member_dists(variable='num_swipes')\n",
        "plot_my_member_dists(variable='num_likes')\n",
        "plot_my_member_dists(variable='like_rate')"
      ],
      "metadata": {
        "id": "HISljl_2eBtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "M7a2ZTgSiyCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# keep  deciders whose (gender, id) with non-0, non-spam likes\n",
        "valid_deciders = like_rates[['decidergender', 'decidermemberid']]\n",
        "\n",
        "df_filtered = df.merge(\n",
        "    valid_deciders,\n",
        "    on=['decidergender', 'decidermemberid'],\n",
        "    how='inner'\n",
        ").copy(deep=True)\n",
        "\n",
        "df_filtered.shape"
      ],
      "metadata": {
        "id": "eVL_pXYgfe4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check filtered interactions df has only users that had valid like_rate\n",
        "len(df_filtered['decidermemberid'].unique())"
      ],
      "metadata": {
        "id": "ABchL7xQi-N9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check retained users have like_rate between (0, 1)\n",
        "check = (\n",
        "    df_filtered\n",
        "    .groupby(['decidergender', 'decidermemberid'])['like']\n",
        "    .mean()\n",
        ")\n",
        "\n",
        "check.min(), check.max()"
      ],
      "metadata": {
        "id": "zXEwOBqsfufN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered[0:5]"
      ],
      "metadata": {
        "id": "SPMsxDQbJnqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This means what's the like rate when users saw each other in their queues compared to when they didn't.\n",
        "# We take all pairs of user IDs and see if they map in reverse.\n",
        "# This result shows that users are more prone to like each other if they saw each other.\n",
        "\n",
        "pairs = set(zip(df_filtered.decidermemberid, df_filtered.othermemberid))\n",
        "df_filtered[\"reciprocal_possible\"] = [\n",
        "    (o, d) in pairs for d, o in zip(df_filtered.decidermemberid, df_filtered.othermemberid)\n",
        "]\n",
        "\n",
        "df_filtered.groupby(\"reciprocal_possible\")[\"like\"].mean()"
      ],
      "metadata": {
        "id": "bnVh_1i2XosG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(pairs)"
      ],
      "metadata": {
        "id": "qn8ycX7Jx8Ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Build"
      ],
      "metadata": {
        "id": "QWuAfSUfztRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered.info()"
      ],
      "metadata": {
        "id": "4FhwlFEP3yP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "KngYVdL4_Q_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter for likes only (like=1 means positive interaction)\n",
        "interactions = df_filtered[df_filtered['like'] == 1].copy(deep=True)\n",
        "\n",
        "# Count interactions per user-item pair (confidence measure)\n",
        "interaction_counts = interactions.groupby(['decidermemberid', 'othermemberid']).size().reset_index(name='interaction_count')\n",
        "\n",
        "# Create user and item mappings\n",
        "unique_users = interaction_counts['decidermemberid'].unique()\n",
        "unique_items = interaction_counts['othermemberid'].unique()\n",
        "\n",
        "user_mapping = {user: idx for idx, user in enumerate(unique_users)}\n",
        "item_mapping = {item: idx for idx, item in enumerate(unique_items)}\n",
        "reverse_user_mapping = {idx: user for user, idx in user_mapping.items()}\n",
        "reverse_item_mapping = {idx: item for item, idx in item_mapping.items()}\n",
        "\n",
        "# Map to matrix indices\n",
        "interaction_counts['user_idx'] = interaction_counts['decidermemberid'].map(user_mapping)\n",
        "interaction_counts['item_idx'] = interaction_counts['othermemberid'].map(item_mapping)\n",
        "\n",
        "print(f\"Users: {len(unique_users)}, Items: {len(unique_items)}\")\n",
        "print(f\"Total interactions: {len(interaction_counts)}\")"
      ],
      "metadata": {
        "id": "FM1joDMB_T4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train-Test Split & Evaluate"
      ],
      "metadata": {
        "id": "O-H8NEk7zxOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters (somewhat arbitrary/relative) - could use grid-search in future to optimise.\n",
        "FACTORS = 64\n",
        "REGULARIZATION = 0.01\n",
        "ITERATIONS = 20\n",
        "ALPHA = 40\n",
        "TEST_SIZE = 0.2\n",
        "\n",
        "# Split data into train and test\n",
        "print(f\"\\nSplitting data: {int((1-TEST_SIZE)*100)}% train, {int(TEST_SIZE*100)}% test\")\n",
        "train_data, test_data = train_test_split(\n",
        "    interaction_counts,\n",
        "    test_size=TEST_SIZE,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Create train matrix with confidence weighting\n",
        "train_confidence = 1 + ALPHA * train_data['interaction_count']\n",
        "train_matrix = csr_matrix(\n",
        "    (train_confidence,\n",
        "     (train_data['user_idx'], train_data['item_idx'])),\n",
        "    shape=(len(user_mapping), len(item_mapping))\n",
        ")\n",
        "\n",
        "# Create test matrix\n",
        "test_confidence = 1 + ALPHA * test_data['interaction_count']\n",
        "test_matrix = csr_matrix(\n",
        "    (test_confidence,\n",
        "     (test_data['user_idx'], test_data['item_idx'])),\n",
        "    shape=(len(user_mapping), len(item_mapping))\n",
        ")\n",
        "\n",
        "# Train the ALS model\n",
        "print(f\"\\nTraining ALS model...\")\n",
        "print(f\"Parameters: factors={FACTORS}, regularization={REGULARIZATION}, \"\n",
        "      f\"iterations={ITERATIONS}, alpha={ALPHA}\")\n",
        "\n",
        "model = AlternatingLeastSquares(\n",
        "    factors=FACTORS,\n",
        "    regularization=REGULARIZATION,\n",
        "    iterations=ITERATIONS,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model.fit(train_matrix)\n",
        "print(\"Training complete!\")\n",
        "\n",
        "# Evaluate model performance\n",
        "print(f\"\\nEvaluating model (Top-10 recommendations)...\")\n",
        "\n",
        "K = 10\n",
        "precisions = []\n",
        "recalls = []\n",
        "\n",
        "# Group test data by user\n",
        "test_grouped = test_data.groupby('user_idx')['item_idx'].apply(list).to_dict()\n",
        "\n",
        "for user_idx, actual_items in test_grouped.items():\n",
        "    if user_idx >= train_matrix.shape[0]:\n",
        "        continue\n",
        "\n",
        "    # Get recommendations\n",
        "    user_items = train_matrix[user_idx]\n",
        "    ids, scores = model.recommend(\n",
        "        user_idx,\n",
        "        user_items,\n",
        "        N=K,\n",
        "        filter_already_liked_items=True\n",
        "    )\n",
        "\n",
        "    recommended_items = ids.tolist()\n",
        "\n",
        "    # Calculate metrics\n",
        "    hits = len(set(recommended_items) & set(actual_items))\n",
        "    precision = hits / K if K > 0 else 0\n",
        "    recall = hits / len(actual_items) if len(actual_items) > 0 else 0\n",
        "\n",
        "    precisions.append(precision)\n",
        "    recalls.append(recall)\n",
        "\n",
        "avg_precision = np.mean(precisions)\n",
        "avg_recall = np.mean(recalls)\n",
        "\n",
        "print(f\"Precision@{K}: {avg_precision:.4f}\")\n",
        "print(f\"Recall@{K}: {avg_recall:.4f}\")"
      ],
      "metadata": {
        "id": "DW-9O2DX6cos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualisation"
      ],
      "metadata": {
        "id": "ZXUhmxckBowM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize results\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# 1. Precision and Recall Distribution\n",
        "ax1 = axes[0, 0]\n",
        "ax1.hist(precisions, bins=10, alpha=0.7, label='Precision', color='blue')\n",
        "ax1.hist(recalls, bins=10, alpha=0.7, label='Recall', color='orange')\n",
        "ax1.axvline(avg_precision, color='blue', linestyle='--',\n",
        "            label=f\"Avg Precision: {avg_precision:.3f}\")\n",
        "ax1.axvline(avg_recall, color='orange', linestyle='--',\n",
        "            label=f\"Avg Recall: {avg_recall:.3f}\")\n",
        "ax1.set_xlabel('Score')\n",
        "ax1.set_ylabel('Frequency')\n",
        "ax1.set_title('Precision & Recall Distribution')\n",
        "ax1.legend()\n",
        "ax1.grid(alpha=0.3)\n",
        "\n",
        "# 2. Interaction density heatmap (sample)\n",
        "ax2 = axes[0, 1]\n",
        "sample_size = min(50, train_matrix.shape[0])\n",
        "sample_matrix = train_matrix[:sample_size, :sample_size].toarray()\n",
        "sns.heatmap(sample_matrix, cmap='YlOrRd', ax=ax2, cbar_kws={'label': 'Confidence'})\n",
        "ax2.set_title(f'User-Item Interaction Heatmap (First {sample_size}x{sample_size})')\n",
        "ax2.set_xlabel('Items')\n",
        "ax2.set_ylabel('Users')\n",
        "\n",
        "# 3. Top users by interaction count\n",
        "ax3 = axes[1, 0]\n",
        "user_interactions = test_data.groupby('user_idx').size().sort_values(ascending=False).head(20)\n",
        "ax3.barh(range(len(user_interactions)), user_interactions.values, color='steelblue')\n",
        "ax3.set_yticks(range(len(user_interactions)))\n",
        "ax3.set_yticklabels([f\"User {idx}\" for idx in user_interactions.index])\n",
        "ax3.set_xlabel('Number of Interactions')\n",
        "ax3.set_title('Top 20 Most Active Users (Test Set)')\n",
        "ax3.invert_yaxis()\n",
        "ax3.grid(axis='x', alpha=0.3)\n",
        "\n",
        "# 4. Model performance summary\n",
        "ax4 = axes[1, 1]\n",
        "ax4.axis('off')\n",
        "\n",
        "summary_text = f\"\"\"\n",
        "Model Performance Summary\n",
        "══════════════════════════\n",
        "\n",
        "Hyperparameters:\n",
        "• Factors: {FACTORS}\n",
        "• Regularization: {REGULARIZATION}\n",
        "• Iterations: {ITERATIONS}\n",
        "• Alpha (confidence): {ALPHA}\n",
        "\n",
        "Dataset:\n",
        "• Total Users: {len(user_mapping):,}\n",
        "• Total Items: {len(item_mapping):,}\n",
        "• Train Interactions: {train_matrix.nnz:,}\n",
        "• Test Interactions: {len(test_data):,}\n",
        "• Sparsity: {(1 - train_matrix.nnz / (train_matrix.shape[0] * train_matrix.shape[1])) * 100:.2f}%\n",
        "\n",
        "Evaluation Metrics (Top-10):\n",
        "• Precision@10: {avg_precision:.4f}\n",
        "• Recall@10: {avg_recall:.4f}\n",
        "\n",
        "Interpretation:\n",
        "• Precision indicates how many recommended\n",
        "  items are relevant\n",
        "• Recall indicates coverage of relevant items\n",
        "\"\"\"\n",
        "\n",
        "ax4.text(0.1, 0.5, summary_text, fontsize=11, verticalalignment='center',\n",
        "        fontfamily='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('recommendation_results.png', dpi=300, bbox_inches='tight')\n",
        "print(\"\\nVisualization saved as 'recommendation_results.png'\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_ocPLmDJ92tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explainability"
      ],
      "metadata": {
        "id": "Td0P6HEuFIe8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the learned user factor matrix (embeddings): model.user_factors shape: (n_users, n_factors)\n",
        "user_embeddings = model.user_factors\n",
        "print(f\"Embedding dimensions: {user_embeddings.shape}\")\n",
        "print(f\"Number of users: {len(user_mapping)}\")\n",
        "print(f\"Embedding size (latent factors): {user_embeddings.shape[1]}\")\n",
        "\n",
        "# Create a dataframe with user IDs and their embeddings\n",
        "embedding_df = pd.DataFrame(user_embeddings)\n",
        "embedding_df['user_idx'] = range(len(user_embeddings))\n",
        "embedding_df['user_id'] = embedding_df['user_idx'].map(reverse_user_mapping)\n",
        "\n",
        "# Merge with demographic information\n",
        "user_demographics = df_filtered[['decidermemberid', 'decidergender', 'deciderdobyear', 'decidersignuptimestamp']].copy()\n",
        "user_demographics = user_demographics.drop_duplicates('decidermemberid')\n",
        "user_demographics.columns = ['user_id', 'gender', 'dob_year', 'signup_timestamp']\n",
        "\n",
        "embedding_df = embedding_df.merge(user_demographics, on='user_id', how='left')\n",
        "\n",
        "# Calculate age and account age\n",
        "current_year = 2025\n",
        "embedding_df['age'] = current_year - embedding_df['dob_year']\n",
        "embedding_df['signup_timestamp'] = pd.to_datetime(embedding_df['signup_timestamp'])\n",
        "embedding_df['account_age_days'] = (pd.Timestamp('2025-01-01') - embedding_df['signup_timestamp']).dt.days\n",
        "\n",
        "# Create age groups\n",
        "embedding_df['age_group'] = pd.cut(embedding_df['age'],\n",
        "                                    bins=[0, 22, 26, 30, 35, 40, 100],\n",
        "                                    labels=['18-22', '23-26', '27-30', '31-35', '36-40', '40+'])\n",
        "\n",
        "# Sample users if dataset is large (for speed)\n",
        "max_users_to_plot = 5000\n",
        "if len(user_embeddings) > max_users_to_plot:\n",
        "    print(f\"Sampling {max_users_to_plot} users from {len(user_embeddings)} for visualization speed...\")\n",
        "    sample_idx = np.random.choice(len(user_embeddings), max_users_to_plot, replace=False)\n",
        "    user_embeddings_sample = user_embeddings[sample_idx]\n",
        "    embedding_df = embedding_df.iloc[sample_idx].reset_index(drop=True)\n",
        "else:\n",
        "    user_embeddings_sample = user_embeddings\n",
        "\n",
        "# Use t-SNE to reduce to 2D (captures non-linear relationships: Using faster parameters: lower perplexity and iterations\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(user_embeddings_sample)//4),\n",
        "            n_iter=500, n_jobs=-1, verbose=0)\n",
        "embeddings_2d_tsne = tsne.fit_transform(user_embeddings_sample)\n",
        "\n",
        "# Use PCA to reduce to 2D (captures linear relationships)\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "embeddings_2d_pca = pca.fit_transform(user_embeddings_sample)\n",
        "\n",
        "# Add to dataframe\n",
        "embedding_df['tsne_x'] = embeddings_2d_tsne[:, 0]\n",
        "embedding_df['tsne_y'] = embeddings_2d_tsne[:, 1]\n",
        "embedding_df['pca_x'] = embeddings_2d_pca[:, 0]\n",
        "embedding_df['pca_y'] = embeddings_2d_pca[:, 1]\n",
        "\n",
        "print(\"Creating visualizations...\")\n",
        "# Create comprehensive visualization\n",
        "fig = plt.figure(figsize=(20, 12))\n",
        "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
        "\n",
        "# 1. t-SNE colored by gender\n",
        "ax1 = fig.add_subplot(gs[0, 0])\n",
        "for gender in embedding_df['gender'].dropna().unique():\n",
        "    mask = embedding_df['gender'] == gender\n",
        "    ax1.scatter(embedding_df[mask]['tsne_x'],\n",
        "               embedding_df[mask]['tsne_y'],\n",
        "               alpha=0.6, s=20, label=gender)\n",
        "ax1.set_title('t-SNE: Colored by Gender', fontsize=12, fontweight='bold')\n",
        "ax1.set_xlabel('t-SNE Dimension 1')\n",
        "ax1.set_ylabel('t-SNE Dimension 2')\n",
        "ax1.legend()\n",
        "ax1.grid(alpha=0.3)\n",
        "\n",
        "# 2. t-SNE colored by age\n",
        "ax2 = fig.add_subplot(gs[0, 1])\n",
        "scatter = ax2.scatter(embedding_df['tsne_x'],\n",
        "                      embedding_df['tsne_y'],\n",
        "                      c=embedding_df['age'],\n",
        "                      cmap='viridis',\n",
        "                      alpha=0.6, s=20)\n",
        "plt.colorbar(scatter, ax=ax2, label='Age')\n",
        "ax2.set_title('t-SNE: Colored by Age', fontsize=12, fontweight='bold')\n",
        "ax2.set_xlabel('t-SNE Dimension 1')\n",
        "ax2.set_ylabel('t-SNE Dimension 2')\n",
        "ax2.grid(alpha=0.3)\n",
        "\n",
        "# 3. t-SNE colored by age group\n",
        "ax3 = fig.add_subplot(gs[0, 2])\n",
        "age_colors = {'18-22': 'purple', '23-26': 'blue', '27-30': 'green',\n",
        "              '31-35': 'orange', '36-40': 'red', '40+': 'brown'}\n",
        "for age_group in embedding_df['age_group'].dropna().unique():\n",
        "    mask = embedding_df['age_group'] == age_group\n",
        "    ax3.scatter(embedding_df[mask]['tsne_x'],\n",
        "               embedding_df[mask]['tsne_y'],\n",
        "               alpha=0.6, s=20, label=age_group,\n",
        "               color=age_colors.get(age_group, 'gray'))\n",
        "ax3.set_title('t-SNE: Colored by Age Group', fontsize=12, fontweight='bold')\n",
        "ax3.set_xlabel('t-SNE Dimension 1')\n",
        "ax3.set_ylabel('t-SNE Dimension 2')\n",
        "ax3.legend(loc='best', fontsize=8)\n",
        "ax3.grid(alpha=0.3)\n",
        "\n",
        "# 4. PCA colored by gender\n",
        "ax4 = fig.add_subplot(gs[1, 0])\n",
        "for gender in embedding_df['gender'].dropna().unique():\n",
        "    mask = embedding_df['gender'] == gender\n",
        "    ax4.scatter(embedding_df[mask]['pca_x'],\n",
        "               embedding_df[mask]['pca_y'],\n",
        "               alpha=0.6, s=20, label=gender)\n",
        "ax4.set_title('PCA: Colored by Gender', fontsize=12, fontweight='bold')\n",
        "ax4.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
        "ax4.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
        "ax4.legend()\n",
        "ax4.grid(alpha=0.3)\n",
        "\n",
        "# 5. PCA colored by age\n",
        "ax5 = fig.add_subplot(gs[1, 1])\n",
        "scatter = ax5.scatter(embedding_df['pca_x'],\n",
        "                      embedding_df['pca_y'],\n",
        "                      c=embedding_df['age'],\n",
        "                      cmap='viridis',\n",
        "                      alpha=0.6, s=20)\n",
        "plt.colorbar(scatter, ax=ax5, label='Age')\n",
        "ax5.set_title('PCA: Colored by Age', fontsize=12, fontweight='bold')\n",
        "ax5.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
        "ax5.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
        "ax5.grid(alpha=0.3)\n",
        "\n",
        "# 6. t-SNE colored by account age\n",
        "ax6 = fig.add_subplot(gs[1, 2])\n",
        "scatter = ax6.scatter(embedding_df['tsne_x'],\n",
        "                      embedding_df['tsne_y'],\n",
        "                      c=embedding_df['account_age_days'],\n",
        "                      cmap='plasma',\n",
        "                      alpha=0.6, s=20)\n",
        "plt.colorbar(scatter, ax=ax6, label='Account Age (Days)')\n",
        "ax6.set_title('t-SNE: Colored by Account Age', fontsize=12, fontweight='bold')\n",
        "ax6.set_xlabel('t-SNE Dimension 1')\n",
        "ax6.set_ylabel('t-SNE Dimension 2')\n",
        "ax6.grid(alpha=0.3)\n",
        "\n",
        "# 7. Gender-Age interaction (t-SNE)\n",
        "ax7 = fig.add_subplot(gs[2, 0])\n",
        "for gender in embedding_df['gender'].dropna().unique():\n",
        "    for age_group in ['18-22', '23-26', '27-30', '31-35']:\n",
        "        mask = (embedding_df['gender'] == gender) & (embedding_df['age_group'] == age_group)\n",
        "        if mask.sum() > 0:\n",
        "            ax7.scatter(embedding_df[mask]['tsne_x'],\n",
        "                       embedding_df[mask]['tsne_y'],\n",
        "                       alpha=0.5, s=15, label=f'{gender} {age_group}')\n",
        "ax7.set_title('t-SNE: Gender-Age Segments', fontsize=12, fontweight='bold')\n",
        "ax7.set_xlabel('t-SNE Dimension 1')\n",
        "ax7.set_ylabel('t-SNE Dimension 2')\n",
        "ax7.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=7)\n",
        "ax7.grid(alpha=0.3)\n",
        "\n",
        "# 8. Density plot\n",
        "ax8 = fig.add_subplot(gs[2, 1])\n",
        "from scipy.stats import gaussian_kde\n",
        "xy = np.vstack([embedding_df['tsne_x'].dropna(), embedding_df['tsne_y'].dropna()])\n",
        "z = gaussian_kde(xy)(xy)\n",
        "ax8.scatter(embedding_df['tsne_x'], embedding_df['tsne_y'],\n",
        "           c=z, s=20, cmap='hot', alpha=0.6)\n",
        "ax8.set_title('t-SNE: Density Heatmap', fontsize=12, fontweight='bold')\n",
        "ax8.set_xlabel('t-SNE Dimension 1')\n",
        "ax8.set_ylabel('t-SNE Dimension 2')\n",
        "ax8.grid(alpha=0.3)\n",
        "\n",
        "# 9. Summary statistics\n",
        "ax9 = fig.add_subplot(gs[2, 2])\n",
        "ax9.axis('off')\n",
        "\n",
        "summary_text = f\"\"\"\n",
        "• Gender breakdown:\n",
        "{embedding_df['gender'].value_counts().to_string()}\n",
        "\n",
        "• Age distribution:\n",
        "  Mean: {embedding_df['age'].mean():.1f} years\n",
        "  Median: {embedding_df['age'].median():.1f} years\n",
        "  Range: {embedding_df['age'].min():.0f}-{embedding_df['age'].max():.0f}\n",
        "\n",
        "• Age groups:\n",
        "{embedding_df['age_group'].value_counts().to_string()}\n",
        "\n",
        "PCA Variance Explained:\n",
        "• PC1: {pca.explained_variance_ratio_[0]:.2%}\n",
        "• PC2: {pca.explained_variance_ratio_[1]:.2%}\n",
        "• Total: {pca.explained_variance_ratio_[:2].sum():.2%}\n",
        "\"\"\"\n",
        "\n",
        "ax9.text(0.05, 0.5, summary_text, fontsize=6, verticalalignment='center',\n",
        "        fontfamily='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
        "\n",
        "plt.suptitle('ALS Embedding Space: Demographic Clustering Analysis',\n",
        "             fontsize=16, fontweight='bold', y=0.995)\n",
        "\n",
        "plt.savefig('embedding_space_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Statistical analysis of clustering\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CLUSTERING ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Analyze separation between genders in embedding space\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "for gender1 in embedding_df['gender'].dropna().unique():\n",
        "    for gender2 in embedding_df['gender'].dropna().unique():\n",
        "        if gender1 < gender2:  # Avoid duplicates\n",
        "            emb1 = user_embeddings_sample[embedding_df['gender'] == gender1]\n",
        "            emb2 = user_embeddings_sample[embedding_df['gender'] == gender2]\n",
        "\n",
        "            # Sample for efficiency\n",
        "            if len(emb1) > 1000:\n",
        "                emb1 = emb1[np.random.choice(len(emb1), 1000, replace=False)]\n",
        "            if len(emb2) > 1000:\n",
        "                emb2 = emb2[np.random.choice(len(emb2), 1000, replace=False)]\n",
        "\n",
        "            # Calculate average distance\n",
        "            if len(emb1) > 0 and len(emb2) > 0:\n",
        "                distances = cdist(emb1, emb2, metric='euclidean')\n",
        "                avg_distance = np.mean(distances)\n",
        "                print(f\"\\nAvg distance between {gender1} and {gender2}: {avg_distance:.3f}\")\n",
        "\n",
        "# Analyze age clustering\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"AGE GROUP SEPARATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "age_groups = embedding_df['age_group'].dropna().unique()\n",
        "for i, ag1 in enumerate(age_groups):\n",
        "    for ag2 in age_groups[i+1:]:\n",
        "        emb1 = user_embeddings_sample[embedding_df['age_group'] == ag1]\n",
        "        emb2 = user_embeddings_sample[embedding_df['age_group'] == ag2]\n",
        "\n",
        "        if len(emb1) > 500:\n",
        "            emb1 = emb1[np.random.choice(len(emb1), 500, replace=False)]\n",
        "        if len(emb2) > 500:\n",
        "            emb2 = emb2[np.random.choice(len(emb2), 500, replace=False)]\n",
        "\n",
        "        if len(emb1) > 0 and len(emb2) > 0:\n",
        "            distances = cdist(emb1, emb2, metric='euclidean')\n",
        "            avg_distance = np.mean(distances)\n",
        "            print(f\"Avg distance between {ag1} and {ag2}: {avg_distance:.3f}\")"
      ],
      "metadata": {
        "id": "wS6gSOHlGJAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deployment"
      ],
      "metadata": {
        "id": "qllO_I3PBu1s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local Inference"
      ],
      "metadata": {
        "id": "toSWQHV6Bx7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_match_queue(user_id, queue_length=20):\n",
        "    # Check if user exists in training data\n",
        "    if user_id not in user_mapping:\n",
        "        print(f\"New user {user_id} - using popularity-based recommendations\")\n",
        "        # For cold start users, return most popular items\n",
        "        item_popularity = np.array(train_matrix.sum(axis=0)).flatten()\n",
        "        top_items_idx = np.argsort(item_popularity)[::-1][:queue_length]\n",
        "\n",
        "        results = []\n",
        "        for item_idx in top_items_idx:\n",
        "            if item_idx in reverse_item_mapping:\n",
        "                original_id = reverse_item_mapping[item_idx]\n",
        "                score = float(item_popularity[item_idx])\n",
        "                results.append((original_id, score))\n",
        "        return results\n",
        "\n",
        "    # Get user index\n",
        "    user_idx = user_mapping[user_id]\n",
        "    user_items = train_matrix[user_idx]\n",
        "\n",
        "    # Get recommendations from model\n",
        "    ids, scores = model.recommend(\n",
        "        user_idx,\n",
        "        user_items,\n",
        "        N=queue_length,\n",
        "        filter_already_liked_items=True\n",
        "    )\n",
        "\n",
        "    # Convert back to original IDs\n",
        "    results = []\n",
        "    for item_idx, score in zip(ids, scores):\n",
        "        original_id = reverse_item_mapping[item_idx]\n",
        "        results.append((original_id, float(score)))\n",
        "    return results\n",
        "\n",
        "\n",
        "# Example usage: Generate a match queue\n",
        "user_to_serve = df_filtered['decidermemberid'].iloc[0]\n",
        "desired_queue_length = 20\n",
        "\n",
        "match_queue = generate_match_queue(user_to_serve, queue_length=desired_queue_length)\n",
        "for rank, (other_user_id, score) in enumerate(match_queue[:desired_queue_length], 1):\n",
        "    user_data = df_filtered[df_filtered['decidermemberid'] == other_user_id].iloc[0] if len(df_filtered[df_filtered['decidermemberid'] == other_user_id]) > 0 else None\n",
        "\n",
        "    if user_data is not None:\n",
        "        gender = user_data.get('decidergender', 'Unknown')\n",
        "        age = 2025 - user_data.get('deciderdobyear', 2000)\n",
        "        print(f\"{rank:2d}. User {other_user_id} | Score: {score:.4f} | {gender}, Age {age}\")\n",
        "    else:\n",
        "        print(f\"{rank:2d}. User {other_user_id} | Score: {score:.4f}\")\n",
        "\n",
        "# Test with different queue lengths\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Queue Length Comparison:\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "for length in [5, 10, 20, 50]:\n",
        "    queue = generate_match_queue(user_to_serve, queue_length=length)\n",
        "    avg_score = np.mean([score for _, score in queue])\n",
        "    print(f\"Queue length {length:2d}: {len(queue)} matches | Avg score: {avg_score:.4f}\")"
      ],
      "metadata": {
        "id": "GEJIKt-H-qIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Serving via API"
      ],
      "metadata": {
        "id": "YvgYu00ZODvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  if type(userdata.get('NGROK')) == str:\n",
        "    print('Found secret')\n",
        "except:\n",
        "  print('Check environment for secrets')"
      ],
      "metadata": {
        "id": "B83H6fkTVfMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (get free token from https://dashboard.ngrok.com/get-started/your-authtoken)\n",
        "ngrok.set_auth_token(userdata.get('NGROK'))\n",
        "\n",
        "# Create Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/recommend', methods=['GET'])\n",
        "def recommend():\n",
        "    \"\"\"\n",
        "    Get recommendations via GET request\n",
        "    Usage: curl \"http://your-url.ngrok-free.app/recommend?user_id=12345&queue_length=10\"\n",
        "    \"\"\"\n",
        "    try:\n",
        "        user_id = int(request.args.get('user_id'))\n",
        "        queue_length = int(request.args.get('queue_length', 20))\n",
        "\n",
        "        # Generate recommendations\n",
        "        match_queue = generate_match_queue(user_id, queue_length=queue_length)\n",
        "\n",
        "        # Format response\n",
        "        recommendations = []\n",
        "        for rank, (other_user_id, score) in enumerate(match_queue, 1):\n",
        "            user_data = df_filtered[df_filtered['decidermemberid'] == other_user_id]\n",
        "\n",
        "            if len(user_data) > 0:\n",
        "                user_data = user_data.iloc[0]\n",
        "                recommendations.append({\n",
        "                    'rank': rank,\n",
        "                    'user_id': int(other_user_id),\n",
        "                    'score': round(float(score), 4),\n",
        "                    'gender': user_data.get('decidergender', 'Unknown'),\n",
        "                    'age': int(2025 - user_data.get('deciderdobyear', 2000))\n",
        "                })\n",
        "            else:\n",
        "                recommendations.append({\n",
        "                    'rank': rank,\n",
        "                    'user_id': int(other_user_id),\n",
        "                    'score': round(float(score), 4)\n",
        "                })\n",
        "\n",
        "        return jsonify({\n",
        "            'user_id': user_id,\n",
        "            'queue_length': queue_length,\n",
        "            'recommendations': recommendations\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({'error': str(e)}), 400\n",
        "\n",
        "# Start Flask in background thread\n",
        "def run_flask():\n",
        "    app.run(port=5000, use_reloader=False)\n",
        "\n",
        "threading.Thread(target=run_flask, daemon=True).start()\n",
        "\n",
        "# Create ngrok tunnel\n",
        "public_url = ngrok.connect(5000)\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"API IS LIVE!\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Public URL: {public_url}\")\n",
        "print(f\"\\Test from CMD:\")\n",
        "print(f'curl \"{public_url}/recommend?user_id={user_to_serve}&queue_length=10\"')\n",
        "print(f\"\\n{'='*70}\\n\")"
      ],
      "metadata": {
        "id": "2al7F6EOOGnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CURL"
      ],
      "metadata": {
        "id": "N5F-QbzYR7JA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl \"https://vegetative-lulu-urogenital.ngrok-free.dev/recommend?user_id=3847776&queue_length=10\""
      ],
      "metadata": {
        "id": "R0cu8y8CR3s8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Export to GitHub"
      ],
      "metadata": {
        "id": "0OpsNOIVaR6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "rNEejEgyaVSa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}